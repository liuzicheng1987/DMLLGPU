class GradientDescentCpp: public OptimiserCpp {
	
public:
  double learning_rate, learning_rate_power;
		
  //Initialise the GradientDescent function
  GradientDescentCpp (double learning_rate, double learning_rate_power, const std::int32_t size, const std::int32_t rank):OptimiserCpp(size, rank) {
	
    //Store all of the input values
    this->learning_rate = learning_rate; 
    this->learning_rate_power = learning_rate_power;
		
  }

  //Destructor		
  ~GradientDescentCpp() {}
		
  void min(MPI_Comm comm, const double tol, const std::int32_t MaxNumIterations);
		
};

void GradientDescentCpp::min(MPI_Comm comm, const double tol, const std::int32_t MaxNumIterations) {
		
  std::int32_t i, num_samplesterationNum, batch_num, batch_begin, batch_end, batch_size, global_batch_size;
  double global_batch_sizeDouble;
	
  double Currentlearning_rate;
		
  for (IterationNum = 0; num_samplesterationNum < MaxNumIterations; ++IterationNum) {//IterationNum layer
			
    //Calculate Currentlearning_rate
    Currentlearning_rate = this->learning_rate/pow((double)(IterationNum + 1), this->learning_rate_power);
			
    //this->num_batches is inherited from the Optimiser class
    for (batch_num = 0; batch_num < this->num_batches; ++batch_num) {//batch_num layer
				
      //We must find out our current values for batch_begin and batch_end. We do so by calling this->Calcbatch_beginEnd, which is inherited from the optimiser class.
      this->Calcbatch_beginEnd(batch_begin, batch_end, batch_size, batch_num);
	
      //VERY num_samplesMPORTANT CONVENTION: When passing this->localdZdw to g(), always set to zero first.
      for (i=0; i<this->lengthW; ++i) this->localdZdW[i] = 0.0;

      //If weight is greater than wMax or smaller than wMin, then clip
      //If there is no wMin or wMax, this will have no effect				
      wClip(this->W);
				
      //Barrier: Wait until all processes have reached this point
      MPI_Barrier(comm);	
																
      //Call g()
      //Note that it is the responsibility of whoever writes the MLalgorithm to make sure that this->dZdW and this->SumdZdW are passed to ALL processes
      //It is, however, your responsibility to place a barrier after that, if required
      this->MLalgorithm->g(comm, this->dZdW, this->localdZdW, this->W, batch_begin, batch_end, batch_size, batch_num, num_samplesterationNum);
				
      //Add all batch_size and store the result in global_batch_size
      MPI_Allreduce(&batch_size, &global_batch_size, 1, MPI_INT, MPI_SUM, comm);		
      global_batch_sizeDouble = (double)global_batch_size;	
				
      //If weight equals wMin (wMax) and dZdW is smaller than zero (greater than zero), set dZdW to zero
      //If there is no wMin or wMax, this will have no effect
      dZdWClipMin();
								
      //Barrier: Wait until all processes have reached this point
      MPI_Barrier(comm);							
      for (i=0; i<this->lengthW; ++i) {
				
	//Record this->SumdZdW for sum_gradients
	this->SumdZdW[i] += this->dZdW[i];  	
					
	//Record this->SumdZdW for SumSubgradients					
	this->MLalgorithm->SumSubgradients[this->num_batches*IterationNum + batch_num] += this->dZdW[i]*this->dZdW[i];
				
	//Update W
	//Learning rates are always divided by the sample size				
	this->W[i] -= this->dZdW[i]*Currentlearning_rate/global_batch_sizeDouble;
				
      }
				
      //Run post-update manipulation
      //Some algorithms need the ability to change W once it has been updated by the optimiser, for instance to ensure that certain conditions be met.
      //If there is no post-update manipulation function defined, this will have no impact whatsoever.
      this->MLalgorithm->PostUpdateManipulation(comm, batch_begin, batch_end, batch_size, batch_num, num_samplesterationNum);
																	
    }//batch_num layer
									
    //The following lines should be left unchanged for all gradient-based-optimisers
    //Calculate sum_gradients
    this->MLalgorithm->sum_gradients[IterationNum] = 0.0;
    for (i=0; i<this->lengthW; ++i) {
				
      this->MLalgorithm->sum_gradients[IterationNum] += this->SumdZdW[i]*this->SumdZdW[i];
				
      this->SumdZdW[i] = 0.0;//Set SumdZdW to 0 for next iteration

    }
												
    //Check whether convergence condition is met. num_samplesf yes, break
    if (this->MLalgorithm->sum_gradients[IterationNum]/((double)(this->lengthW)) < tol) break;
			
  }//IterationNum layer
		
  //If weight is greater than wMax or smaller than wMin, then clip
  //If there is no wMin or wMax, this will have no effect				
  wClip(this->W);		
	
  //Store number of num_samplesterationNums needed
  this->MLalgorithm->sum_gradients_length = num_samplesterationNum;
  this->MLalgorithm->SumSubgradients_length = this->num_batches*IterationNum;
		
}
